# 9장 웹 로봇

## 9.1 크롤러와 크롤링

- 크롤러: 웹 페이지들을 순회하면서 데이터를 가져오는 로봇
- 크롤링 방식
    1. 크롤러에게 출발지점 - 루트 집합을 주면 해당 루트 집합을 기준으로 연결된 웹 페이지들을 순회한다.
    2. 크롤러는 웹 페이지 안에 들어있는 URL을 파싱해서 해당 URL을 순차적으로 방문한다.
    
    ![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%209ba0e125d56e4e4a822e2f003d578649/Untitled.png)
    

- 웹 페이지 안의 URL이 순환구조로 되어있으면 영원히 순환하여 방문하기 때문에 위험하다.
- 중복된 페이지를 계속 방문하는것도 리소스 낭비인데 이 문제를 해결하기 위해 아래 4가지 방법을 사용한다.
    1. 트리와 해시 테이블
        - 방문한 URL을 해시 테이블에 저장해서 확인한다.
    2. 느슨한 존재 비트맵
        - 존재 비트 배열 이라는 느슨한 자료구조로 확인한다.
        - 방문 안한 URL도 방문했다고 할 수 있지만 대충 넘어간다.
    3. 체크 포인트
        - 방문한 URL 목록을 디스크에 저장시킨다.
    4. 파티셔닝
        - 여러개의 로봇을 분산처리시키면서 서로 URL정보를 주고받는다.
        
- URL이 별칭을 가질 경우, 동일한 페이지를 방문한건지 체크할때 헷갈릴 수 있다.
    
    ![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%209ba0e125d56e4e4a822e2f003d578649/Untitled%201.png)
    
    - 위와같은 애매한 경우들을 처리하기 위해 대부분의 웹 로봇은 URL을 정규화 한다.
    - 정규화 예제
        - 포트번호가 없는경우 자동으로 :80 붙임
        - 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환
        - #태크 제거
        
- 파일 시스템 링크 탐색중 순환이 일어날 수 있다.
    - / → /index.html → / → index.html 형식의 무한 순환에 빠질 수 있다.
    - 이런 형식은 봇을 부수기위해 악의적으로 만들어졌을 수도 있고 아무 생각없이 만들었을 수도 있다.
    
- 크롤러가 무한히 순환하는 것을 방지하기 위한 방법들
    1. URL 정규화
        - 중복된 URL 방문을 피함
    2. 너비 우선 크롤링
        - BFS 방식으로 크롤링하면 한 웹페이지만 계속 파고드는걸 방지할수있다.
    3. 스로틀링 
        - 특정 웹 사이트에 최대 몇번만 접근하게 설정한다.
    4. URL 크기 제한
        - URL이 너무 길면 크롤링 안함
    5. URL/ 사이트 블랙리스트
        - 악의적인 봇 방해 사이트는 블랙리스트에 올림
    6. 패턴 발견
        - 동일한 패턴이 계속 반복되면 방문 안함
    7. 콘텐츠 지문(fingerprint)
        - 페이지에 지문을 찍어서 고유한 페이지인지 확인함
    8. 사람이 모니터링함
        - 인간이 로그 등으로 직접 봇 상태 확인
    

## 9.2 로봇의 HTTP

- 로봇은 HTTP 요청을 보낼때 자기가 로봇인걸 알려줘야 한다.
- 요청 헤더
    - User-Agent
        - 로봇의 이름 설정
    - From
        - 로봇의 관리자 이메일 주소
    - Accept
        - 서버에게 어떤 미디어 타입을 보내도 되는지 설정
    - Referer
        - 현재 요청을 보내는 문서의 URL
    - Host
        - 가상 호스팅 일수도 있으므로 설정 필요

- 로봇은 아래 응답을 해석할 수 있어야 한다.
    - 상태코드 (header)
    - 엔터티 (body)

## 9.3 부적절하게 동작하는 로봇들

- 로봇이 발생시킬 수 있는 문제점
    1. 미친듯이 많은 HTTP 요청으로 서버를 과부하 시킴
    2. 오랜된 URL을 방문해서 서버에 에러 로그를 많이 쌓아줌
    3. 길고 잘못된 URL을 요청해서 웹 서버 처리 능력에 영향을 줌
    4. 사적인 컨텐츠를 긁어올 수도 있음
    5. 게이트웨이 애플리케이션의 콘텐츠를 요청할 수도 있음
    

## 9.4 로봇 차단하기

- robot.txt 를 서버 문서 루트에 만들어서 로봇의 접근을 제한할 수 있다.
    
    ([https://www.coupang.com/robots.txt](https://www.coupang.com/robots.txt))
    
- 로봇은 반드시 robot.txt를 확인하고 처리해야한다.
- robot.txt가 없으면 로봇마음대로 다 가져와도 된다.
- robot.txt 파일 포맷
    - User-Agent
        - 해당 이름의 로봇만 접근가능
    - Disallow / Allow
        - 접근 불가/가능 URL 설정
    - 다른 커스텀 필드가 있을 수도 있다. 로봇이 이해 못하는 필드는 무시해도 된다.
- robot.txt는 Cache-Control의 Expires 헤더대로 캐시기간을 설정해야한다.

- robot.txt 대신 HTML 문서의 META 태그에 로봇 접근 설정을 할 수 있다.
    - <META NAME = “ROBOT” CONTENT = “NOINEDX”>
        - 로봇이 이 페이지를 처리하지 않게 함
    - <META NAME= “ROBOT” CONTENT = “NOFOLLOW”>
        - 이 페이지가 링크한 페이지를 크롤링하지 않게 함
    - <META NAME = “ROBOT” CONTENT = “INDEX”>
        - 이 페이지 크롤링 해도 됨
    - <META NAME= “ROBOT” CONTENT = “FOLLOW”>
        - 이 페이지가 링크한 페이지 크롤링 해도 됨
    

## 9.5 로봇 에티켓

- 로봇이 지켜야하는 에티켓이 있다. (작은 크롤러도 해당된다.)

1. User-Agent 로 신원을 밝혀라
2. 로봇의 IP로 역방향 DNS를 할 수 있게 하라 (로봇의 조직을 밝혀라)
3. HTTP 요청시, 연락가능한 이메일을 제공하라
4. 로봇 운영자를 고용해서 로봇이 나쁜짓을 안하는지 감시하라
5. 로봇에 대한 문의에 대응하라
6. 로봇의 진행상황을 추적하고 로깅해서 근거없는 불평에 대해 방어하라
7. 로봇이 함정에 빠지지 않도록 조정하라
8. URL를 필터링 하라
9. 동적 URL을 필터링 하라
10. Accept 관련 헤더로 필터링 하라
11. robot.txt를 따르라
12. 특정 사이트에 너무 자주 접근하지 않게 제어하라
13. 모든 HTTP 상태코드 응답에 대응하게 만들어라
14. URL를 정규화 해서 중복된 URL 요청을 피하라
15. 순환, 함정을 피하고 블랙리스트를 관리해라
16. 로봇이 얼마나 많은 메모리를 사용할지 파악하라
17. 네트워크 대역폭이 얼마나 많이 필요한지 파악하라
18. 로봇이 크롤링하는데 얼마나 오래걸리는지 파악하라
19. 대규모 크롤링을 위해서는 로봇을 분할시킬 필요가 있다.
20. 로봇이 제대로 동작하는지 완벽하게 테스트해라
21. 로봇이 실패한 경우 처음부터 다시 크롤링하지 않게 체크포인트를 생성하라
22. 로봇이 실패해도 계속 동작하게 설계하라
23. 로봇에 화난 사람들에 대응하기 위해 로봇에 대한 정책 페이지를 만들어라
24. 로봇에 화난 사람들을 이해하라
25. 로봇에 화난 사람들에게 즉각적으로 대응해줘라

## 9.6 검색 엔진

- 요즘 검색엔진은 아래 그림과 같이 크롤러가 데이터를 수집해서 풀 텍스트 색인 데이터베이스를 만들고 
검색을 하면 해당 데이터베이스에서 검색한다.

![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%209ba0e125d56e4e4a822e2f003d578649/Untitled%202.png)

- 풀 텍스트 색인은 각 단어들이 어떤 문서에 들어있는지 저장한다.

![Untitled](9%E1%84%8C%E1%85%A1%E1%86%BC%20%E1%84%8B%E1%85%B0%E1%86%B8%20%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA%209ba0e125d56e4e4a822e2f003d578649/Untitled%203.png)

- 검색 하면 HTTP 요청으로 응답을 받아오고 정렬같은걸 수행한다. 이때, 관련도로 정렬하는 랭킹 시스템이나 검색 알고리즘, 크롤링 팁같은건 검색엔진의 비밀이다.
- 광고 사이트같은대서 검색 순위를 높이고자 검색어에 해당하는 단어만 많이 써놓는 방식의 사기수법을 스푸핑이라고 한다.